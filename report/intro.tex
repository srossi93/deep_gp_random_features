
\section{Introduction}

The rapid progress of machine learning in the last few years are largely the result of advances in deep learning, combined with the availability of large datasets, fast GPUs and distributed computing solutions (\citet{zoe}). For instance, there are models that can recognize images with an accuracy that rivals that of humans (\citet{dgp}) and this is leading to revolutions in several domains such as autonomous transportation and medical image understanding.


All of these systems, though, currently use \textbf{supervised learning} in which the machine is trained with inputs labeled by humans. The challenge is to let machines learn from raw, unlabeled stream of data. This is known as \textbf{unsupervised learning}; Artificially Intelligent Systems nowadays do not possess ``common sense'', which humans acquire by observing the world, acting in it, and understanding the physical constraints of it. Geo  Hinton, who is a famous professor of ML at the University of Toronto, has said

\quoteauthor{
When we’re learning to see, nobody’s telling us what the right answers are — we just look. Every so often, your mother says “that’s a dog”, but that’s very little information. You’d be lucky if you got a few bits of information — even one bit per second — that way. The brain’s visual system has $10^{14}$  neural connections. And you only live for $10^9$ seconds. So it’s no use learning  one bit per second. You need more like $10^5$ bits per second. And there’s only one place you can get that much information: from the input itself.}{Georey Hinton}{1996 (quoted in (Gorder 2006))}

Two of the most interesting problem concerning unsupervised learning are \textbf{dimensionality reduction} and \textbf{clustering}.

%Approaches to unsupervised learning will be reviewed. This presentation assumes some familiarity with the basic concepts of deep learning.

%Given their impressive performance on machine learning and pattern recognition tasks, Deep Neural Networks (\dnn{s}) have recently attracted a considerable deal of attention in several applied domains such as computer vision and natural language processing; see, e.g., \citet{Lecun15Nature} and references therein.
%Deep Gaussian Processes \citep[\dgp{s};][]{Damianou13} alleviate the outstanding issue characterizing \dnn{s} of having to specify the number of units in hidden layers by implicitly working with infinite representations at each layer.
%From a generative perspective, \dgp{s} transform the inputs using a cascade of Gaussian Processes \citep[\gp{s};][]{Rasmussen06} such that the output of each layer of \gp{s} forms the input to the \gp{s} at the next layer, effectively implementing a deep probabilistic nonparametric model for compositions of functions \citep{Neal96,Duvenaud14}.

%% The flexibility and expressive power of \dgp{s}, however, comes at the expense of having to deal with a model for which learning is extremely difficult.
%Because of their probabilistic formulation, it is natural to approach the learning of \dgp{s} through Bayesian inference techniques; however, the application of such techniques to learn \dgp{s} leads to various forms of intractability.
%A number of contributions have been proposed to recover tractability, extending or building upon the literature on approximate methods for \gp{s}.
%Nevertheless, only few works leverage one of the key features that arguably make \dnn{s} so successful, that is being scalable through the use of mini-batch-based learning \citep{Hensman14,Dai15,Bui16}.
%Even among these works, there does not seem to be an approach that is truly applicable to large-scale problems, and practical beyond only a few hidden layers.

%In this paper, we develop a practical learning framework for \dgp models that significantly improves the state-of-the-art on those aspects.
%In particular, our proposal introduces two sources of approximation to recover tractability, while (i) scaling to large-scale problems, (ii) being able to work with moderately deep architectures, and (iii) being able to accurately quantify uncertainty. % \noteKC{Generalize.}
%The first is a model approximation, whereby the \gp{s} at all layers are approximated using random feature expansions \citep{Rahimi08}; the second approximation relies upon stochastic variational inference to retain a probabilistic and scalable treatment of the approximate \dgp model.

%We show that random feature expansions for \dgp models yield Bayesian \dnn{s} with low-rank weight matrices, and the expansion of different covariance functions results in different \dnn activation functions, namely trigonometric for the Radial Basis Function (\rbf) covariance, and Rectified Linear Unit (\relu) functions for the \arccosine covariance.
%In order to retain a probabilistic treatment of the model % when scaling to large datasets, we employ stochastic variational inference techniques.
%% In particular,
%we adapt the work on variational inference for \dnn{s} and variational autoencoders \citep{Graves11,Kingma14} using mini-batch-based stochastic gradient optimization, which can exploit GPU and distributed computing.
%In this respect, we can view the probabilistic treatment of \dgp{s} approximated through random feature expansions as a means to specify sensible and interpretable priors for probabilistic \dnn{s}.
%Furthermore, unlike popular inducing points-based approximations for \dgp{s}, the resulting learning framework does not involve any matrix decompositions in the size of the number of inducing points, but only matrix products.
%We implement our model in TensorFlow \citep{MartinAbadi15}, which allows us to rely on automatic differentiation to apply stochastic variational inference.




%% This is due to the fact that the nonlinearity introduced by each layer leads to the necessity to solve highly nontrivial integrals which makes it difficult to tractably propagate the uncertainty throughout the layers.
%% ,
%% An additional complication stems from the strong coupling of the \gp{s} at all layers, which are further parameterized by covariance parameters to be optimized or inferred.
% \noteEB{How about the inherent complexity of GPs?}




%% %% An attractive feature of \dgp{s} is that, unlike standard \dnn{s}, their probabilistic formulation allows for a principled way to tackle quantification of uncertainty and model selection, which is desirable when employing deep models in practice. % \noteKC{To be cleared up}
%% %% \noteEB{But so do Bayesian DNNs, unclear why DGPs are different here}

%% %% While approximations have been developed to recover tractability in general Bayesian inference tasks \citep{Candela05,Titsias09,Hensman15,Dezfouli15}, their application to \dgp{s} remains nonetheless challenging.



%% %% Although our approach is amenable to any stationary covariance function, we develop it in detail for the squared exponential and arc-cosine covariance functions.
%% Applying random feature expansions to \dgp{s}, we obtain \dnn{s} with low-rank weight matrices.
%% Feature expansions of different covariance functions result in different \dnn activation functions, namely trigonometric for the radial basis function covariance, and rectified linear unit functions for the arc-cosine covariance.
%% By virtue of the connection to \dnn{s}, we are able to employ scalable optimization and inference techniques that have been developed in the literature to train \dnn{s}, allowing the proposed \dgp model to scale to large data in the same way as \dnn{s}.

%% In order to retain a probabilistic treatment of the model when scaling to large datasets, we employ stochastic variational inference techniques.
%% In particular, we adapt the work on variational inference for \dnn{s} and variational autoencoders \citep{Graves11,Kingma14,Rezende14} using mini-batch-based stochastic gradient optimization for the proposed \dgp model, which can exploit GPU and distributed computing.
%% We implement the model in TensorFlow \citep{MartinAbadi15}, which allows us to rely on automatic differentiation during the optimization procedure.
%% %% The impact of the variational approximation, as opposed to characterizing the full posterior distribution over all model parameters, is the most difficult to assess.
%% In the supplementary material, we also illustrate the quality of our approximation by comparing the variational approximation of the proposed model with % a run of a
%% the inference resulting from the application of Markov chain Monte Carlo (\mcmc) methods that we develop for two-layer \dgp regression models.

%Although having to select the appropriate number of random features goes against the nonparametric formulation favored in \gp models, the level of approximation can be tuned based on constraints on running time or hardware.
%Most importantly, the random feature approximation enables us to develop a learning framework for \dgp{s} which significantly advances the state-of-the-art.
%We extensively demonstrate the effectiveness of our proposal on a variety of regression and classification problems by comparing it with \dnn{s} and other state-of-the-art approaches to infer \dgp{s}.
%The results indicate that for a given \dgp architecture, our proposal is consistently faster at achieving lower errors compared to the competitors.
%Another key observation is that the proposed \dgp outperforms \dnn{s} trained with dropout on quantification of uncertainty metrics.

%We focus part of the experiments on large-scale problems, such as \mnisteight digit classification and the \airline dataset, which contain over $8$ and $5$ million observations, respectively.
%Only very recently there have been attempts to demonstrate performance of \gp models on such large data sets \citep{Wilson16,Krauth17}, and our proposal is on par with these latest \gp methods.
%Furthermore, we obtain impressive results when employing our learning framework to \dgp{s} with moderate depth (few tens of layers) on the \airline dataset.
%We are not aware of any other \dgp models having such depth that can achieve comparable performance when applied to datasets with millions of observations.
%Crucially, we obtain all these results by running our algorithm on a single machine without GPUs, but our proposal is designed to be able to exploit GPU and distributed computing to significantly accelerate our deep probabilistic learning framework (see supplement for experiments in distributed mode). % ; for completeness we also report some prelimiary results on the performance of an asynchronous distributed version of our code in the supplementary material.

%% Such an undertaking would not have been computationally feasible using a regular \gp model, while other \dgp approaches do not converge as quickly as our technique.

% \noteMF{Change - Furthermore, the inclusion of an asynchronous implementation of stochastic gradient optimization allows us to exploit large-scale computing facilities using distributed updates, showing that we can truly scale \dgp{s} to tractably tackle unprecedently large amounts of data.} \noteKC{Now moved to supplementary.}% \notePM{To be discussed, but I'd be careful here. Even the mnsit 8M is not that big, so we cannot really claim we learn from big data. What we do is that we exploit parallelism to lower training time, provided that our experiments really show this.}

%In summary, the most significant contributions of this work are as follows:
%% \begin{itemize}
%% \item
%(i) we propose a novel approximation of \dgp{s} based on random feature expansions that we study in connection with \dnn{s};
%(ii) we demonstrate the ability of our proposal to systematically outperform state-of-the-art methods to carry out inference in \dgp models, especially for large-scale problems and moderately deep architectures;
%(iii) we validate the superior quantification of uncertainty offered by \dgp{s} compared to \dnn{s}.

%% \noteEB{May be we can focus on why DGPs are good. Flexible (non-parametric) approaches to quantifying uncertainty in DNNs. By using random features, we have a state-of-the-art way to train these models. While the resulting model is a Bayesian DNNs, we still mantain the theoretical connection with DGPs, which is important for prior setting, variable initialization and posterior analysis. In fact, we do use these tree advantages in our experiments!}

%% The paper is organized as follows.
%% The rest of this section reports the related work.
%% Sec.~2 introduces \dgp{s} and demonstrates how covariance functions can be approximated with random features.
%% In Sec.~3, we propose our construction of \dgp{s} using such approximations, and show how the resulting model can be learned using stochastic variational inference.
%% Sec.~4 reports an extensive evaluation of our proposal and a thorough comparison with state-of-the-art approaches to learn \dgp{s} and \dnn{s}, while Sec.~5 concludes the paper.
%% \noteEB{No one really reads the paragraph above, we can get rid of it and make some room.}
