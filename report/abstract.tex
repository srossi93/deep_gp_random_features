\begin{abstract}
Deep Gaussian Processes are modern deep probabilistic models that allow to model complex relationships in observed data. Deep Gaussian Processes are extremely complex to train and we recently made some significant advances in making these scalable. The use of Deep Gaussian Processes has been limited to the case of supervised problems. This project will focus on extending the use of Deep Gaussian Processes to unsupervised problems, notably clustering problems.

TODO
%The composition of multiple Gaussian Processes as a Deep Gaussian Process (\dgp) %enables a deep probabilistic nonparametric approach to flexibly tackle complex %machine learning problems with sound quantification of uncertainty.
%Existing inference approaches for \dgp models have limited scalability and are %notoriously cumbersome to construct.
%In this work we introduce a novel formulation of \dgp{s} based on random feature %expansions that we train using stochastic variational inference.
%This yields a practical learning framework which significantly advances the %state-of-the-art in inference for \dgp{s}, and enables accurate quantification of %uncertainty. % compared to regularized deep neural networks.
%We extensively showcase the scalability and performance of our proposal on several %datasets with up to $8$ million observations, and various \dgp architectures with up %to $30$ hidden layers.
\end{abstract}
